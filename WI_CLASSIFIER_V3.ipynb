{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "WI_CLASSIFIER_V3.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/agarr3/vajra-new/blob/main/WI_CLASSIFIER_V3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Er3dkSvCNWw7"
      },
      "source": [
        "import numpy as np # linear algebra\n",
        "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
        "import os\n",
        "from tqdm import tqdm_notebook as tqdm\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "# Neural networks can be constructed using the torch.nn package.\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.utils.data.sampler import SubsetRandomSampler\n",
        "from torch.utils.data import Dataset\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "import IPython.display as ipd\n",
        "\n",
        "import random"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bNqpIVMZNjL4"
      },
      "source": [
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Assuming that we are on a CUDA machine, this should print a CUDA device:\n",
        "\n",
        "print(device)\n",
        "\n",
        "#settings\n",
        "batch_size = 128\n",
        "validation_split = .3\n",
        "shuffle_dataset = True\n",
        "random_seed= 42\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "n_epochs = 30\n",
        "print_every = 1\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "seq_len=200\n",
        "\n",
        "DATA_BASE_PATH = \"/content/gdrive/My Drive/vajra/AE_HYBRID_LSTM_V1/CHROMA\"\n",
        "DATA_SONG_DIR = \"originals\"\n",
        "DATA_IMG_DIR = \"originals\"\n",
        "input_bins = 12\n",
        "BASE_PATH = \"/content/gdrive/My Drive/vajra/WI_CLASSIFIER_V1/ORIGINAL_CHROMA/\"\n",
        "\n",
        "# DATA_BASE_PATH = \"/content/gdrive/My Drive/vajra/AE_HYBRID_LSTM_V1/CHROMA\"\n",
        "# DATA_SONG_DIR = \"Activations_64\"\n",
        "# DATA_IMG_DIR = \"Activations_64\"\n",
        "# input_bins = 64\n",
        "# BASE_PATH = \"/content/gdrive/My Drive/vajra/WI_CLASSIFIER_V1/LAYER1_64_CHROMA/\"\n",
        "\n",
        "# DATA_BASE_PATH = \"/content/gdrive/My Drive/vajra/AE_HYBRID_LSTM_V1/CHROMA\"\n",
        "# DATA_SONG_DIR = \"Activations_128\"\n",
        "# DATA_IMG_DIR = \"Activations_128\"\n",
        "# input_bins = 128\n",
        "# BASE_PATH = \"/content/gdrive/My Drive/vajra/WI_CLASSIFIER_V1/LAYER2_128_CHROMA/\"\n",
        "\n",
        "# DATA_BASE_PATH = \"/content/gdrive/My Drive/vajra/AE_HYBRID_LSTM_V1/CHROMA\"\n",
        "# DATA_SONG_DIR = \"reconstructions\"\n",
        "# DATA_IMG_DIR = \"reconstructions\"\n",
        "# input_bins = 12\n",
        "# BASE_PATH = \"/content/gdrive/My Drive/vajra/WI_CLASSIFIER_V1/reconstructions_CHROMA/\"\n",
        "\n",
        "\n",
        "TEST_DATA_BASE_PATH = \"/content/gdrive/My Drive/vajra/AE_HYBRID_LSTM_V1\"\n",
        "TEST_DATA_SONG_DIR = \"reconstructions\"\n",
        "TEST_DATA_IMG_DIR = \"reconstructions\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8HiwoaWykxXa"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VHJ85uiQb6hN"
      },
      "source": [
        "print(os.path.join(DATA_BASE_PATH, DATA_IMG_DIR, \"Western_numpy\"))\n",
        "             \n",
        "train_files = []\n",
        "exclusion = [\"098567.npy\",\"098569.npy\", '098565.npy']\n",
        "for r,d, fileList in os.walk(os.path.join(DATA_BASE_PATH, DATA_IMG_DIR, \"training\", \"Western_numpy\")):\n",
        "  for file in fileList:\n",
        "    if(file not in exclusion):\n",
        "      train_files.append([file,file, \"Western_numpy\"])\n",
        "    else:\n",
        "      print(file)\n",
        "\n",
        "random.seed(1234)\n",
        "train_files = random.sample(train_files, len(train_files))\n",
        "\n",
        "for r,d, fileList in os.walk(os.path.join(DATA_BASE_PATH, DATA_IMG_DIR, \"training\", \"Indian_numpy\")):\n",
        "  for file in fileList:\n",
        "    if(True):\n",
        "      train_files.append([file,file, \"Indian_numpy\"])\n",
        "    else:\n",
        "      print(file)\n",
        "\n",
        "train_data = pd.DataFrame(train_files, columns=['Images','songs','labels'])\n",
        "train_data.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UjrNuyVQdsrV"
      },
      "source": [
        "train_data.groupby(\"labels\").count()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k0I0Wfo6dibK"
      },
      "source": [
        "test_files = []\n",
        "exclusion = [\"098567.npy\",\"098569.npy\", '098565.npy']\n",
        "for r,d, fileList in os.walk(os.path.join(DATA_BASE_PATH, DATA_IMG_DIR, \"test\", \"Western_numpy\")):\n",
        "  for file in fileList:\n",
        "    if(file not in exclusion):\n",
        "      test_files.append([file,file, \"Western_numpy\"])\n",
        "    else:\n",
        "      print(file)\n",
        "\n",
        "random.seed(1234)\n",
        "test_files = random.sample(test_files, len(test_files))\n",
        "\n",
        "for r,d, fileList in os.walk(os.path.join(DATA_BASE_PATH, DATA_IMG_DIR, \"test\", \"Indian_numpy\")):\n",
        "  for file in fileList:\n",
        "    if(True):\n",
        "      test_files.append([file,file, \"Indian_numpy\"])\n",
        "    else:\n",
        "      print(file)\n",
        "\n",
        "test_data = pd.DataFrame(test_files, columns=['Images','songs','labels'])\n",
        "test_data.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ERDEfpHgbWUr"
      },
      "source": [
        "test_data.groupby(\"labels\").count()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M6HnBomQaKKE"
      },
      "source": [
        "test_idx = random. randint(0,1000)\n",
        "test_img_path = os.path.join(DATA_BASE_PATH, DATA_IMG_DIR, \"training\", train_data.loc[test_idx, 'labels'], train_data.loc[test_idx, 'Images'])\n",
        "print(test_img_path)\n",
        "\n",
        "numpy_arr = np.load(test_img_path)\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.pcolormesh(numpy_arr)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uj3QNTtlAgAw"
      },
      "source": [
        "test_img_path = '/content/gdrive/My Drive/Kaggle/GTZAN/Data/images_original/reggae/reggae00016.png'\n",
        "test_image = Image.open(test_img_path)\n",
        "test_image = test_image.convert('RGB').convert('L')\n",
        "test_image"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hdIZbFJMksHU"
      },
      "source": [
        "lb = LabelEncoder()\n",
        "train_data['encoded_labels'] = lb.fit_transform(train_data['labels'])\n",
        "train_data.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jzlmts7_dzuH"
      },
      "source": [
        "test_data['encoded_labels'] = lb.transform(test_data['labels'])\n",
        "test_data.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XmjbpBD145y5"
      },
      "source": [
        "classes = ('Indian_numpy', 'Western_numpy')\n",
        "train_data.groupby(by=['labels','encoded_labels']).count()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ATSufZlJeui-"
      },
      "source": [
        "test_data.groupby(by=['labels','encoded_labels']).count()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TkxtVkXb4FSl"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "train_data = train_data.sample(frac=1)\n",
        "test_data = test_data.sample(frac=1)\n",
        "\n",
        "\n",
        "#df_train, df_test = train_test_split(data, test_size=validation_split, stratify=data[[\"labels\"]])\n",
        "\n",
        "df_train = train_data.reset_index(drop=True)\n",
        "df_test = test_data.reset_index(drop=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iBfonfYQ5Eli"
      },
      "source": [
        "df_train.groupby(by=['labels','encoded_labels']).count()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9on2cPml5F6y"
      },
      "source": [
        "df_test.groupby(by=['labels','encoded_labels']).count()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rGP82fNHg0uW"
      },
      "source": [
        "transform_normal = transforms.Compose(\n",
        "    [transforms.ToTensor(),\n",
        "     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
        "\n",
        "transform = transforms.Compose(\n",
        "    [transforms.ToTensor()])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yrN_Gvkkg_5I"
      },
      "source": [
        "class MEL_Dataset(Dataset):\n",
        "    def __init__(self, img_data,img_path,transform=None):\n",
        "        self.img_path = img_path\n",
        "        self.transform = transform\n",
        "        self.img_data = img_data\n",
        "        \n",
        "    def __len__(self):\n",
        "        return len(self.img_data)\n",
        "    \n",
        "    def __getitem__(self, index):\n",
        "        img_name = os.path.join(self.img_path,self.img_data.loc[index, 'labels'],\n",
        "                                self.img_data.loc[index, 'Images'])\n",
        "        image = np.load(img_name)\n",
        "        image = image[:,:seq_len]\n",
        "        label = torch.tensor(self.img_data.loc[index, 'encoded_labels'])\n",
        "        if self.transform is not None:\n",
        "            image = self.transform(image)\n",
        "        return image, label\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y4BFIETP8vvE"
      },
      "source": [
        "train_dataset = MEL_Dataset(df_train,os.path.join(DATA_BASE_PATH, DATA_IMG_DIR, \"training\"),transform)\n",
        "test_dataset = MEL_Dataset(df_test,os.path.join(DATA_BASE_PATH, DATA_IMG_DIR, \"test\"),transform)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jh1DuM6M-T9k"
      },
      "source": [
        "trainloader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size)\n",
        "testloader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zt0fvpWuCLQG"
      },
      "source": [
        "def img_display(img):\n",
        "    #img = img / 2 + 0.5     # unnormalize\n",
        "    npimg = img.numpy()\n",
        "    npimg = np.transpose(npimg, (1, 2, 0))\n",
        "    return npimg.squeeze()\n",
        "\n",
        "def mel_display(S_DB):\n",
        "    #print(S_DB.shape)\n",
        "    S_DB = S_DB.numpy()\n",
        "    array = S_DB*-1\n",
        "    #print(array.max())\n",
        "    array *= (255.0/array.max())\n",
        "    array = 255 - array\n",
        "\n",
        "    array = np.transpose(array, (1, 2, 0))\n",
        "    return array.squeeze()\n",
        "\n",
        "def imshow(img):\n",
        "    #img = img / 2 + 0.5     # unnormalize\n",
        "    npimg = img.numpy()\n",
        "    array = npimg*-1\n",
        "    #print(array.max())\n",
        "    array *= (255.0/array.max())\n",
        "    array = 255 - array\n",
        "    array = np.transpose(array, (1, 2, 0))\n",
        "    plt.imshow(array.squeeze(), interpolation='nearest', aspect='auto')\n",
        "    plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fv4aFL9d_Vrc"
      },
      "source": [
        "# get some random training images\n",
        "dataiter = iter(trainloader)\n",
        "images, labels = dataiter.next()\n",
        "arthopod_types = {0: 'Indian_numpy', 1: 'Western_numpy', 2: 'country', 3: 'disco', 4: 'hiphop', 5: 'jazz', 6: 'metal', 7: 'pop', 8: 'reggae', 9: 'rock'}\n",
        "# Viewing data examples used for training\n",
        "fig, axis = plt.subplots(3, 5, figsize=(15, 10))\n",
        "for i, ax in enumerate(axis.flat):\n",
        "    with torch.no_grad():\n",
        "        image, label = images[i], labels[i]\n",
        "        ax.imshow(mel_display(image), interpolation='nearest', aspect='auto') # add image\n",
        "        ax.set(title = f\"{arthopod_types[label.item()]}\") # add label"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M8voieJQGmsI"
      },
      "source": [
        "# get some random training images\n",
        "dataiter = iter(testloader)\n",
        "images, labels = dataiter.next()\n",
        "arthopod_types = {0: 'Indian_numpy', 1: 'Western_numpy', 2: 'country', 3: 'disco', 4: 'hiphop', 5: 'jazz', 6: 'metal', 7: 'pop', 8: 'reggae', 9: 'rock'}\n",
        "# Viewing data examples used for training\n",
        "fig, axis = plt.subplots(3, 5, figsize=(15, 10))\n",
        "for i, ax in enumerate(axis.flat):\n",
        "    with torch.no_grad():\n",
        "        image, label = images[i], labels[i]\n",
        "        ax.imshow(mel_display(image), interpolation='nearest', aspect='auto') # add image\n",
        "        ax.set(title = f\"{arthopod_types[label.item()]}\") # add label"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2o_uZfjnCX1V"
      },
      "source": [
        "class Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.conv1 = nn.Conv2d(1, 6, 5)\n",
        "        self.pool = nn.MaxPool2d(2, 2)\n",
        "        self.conv2 = nn.Conv2d(6, 16, 3)\n",
        "        if input_bins == 12:\n",
        "          self.fc1 = nn.Linear(768, 120)\n",
        "        if input_bins == 64:\n",
        "          self.fc1 = nn.Linear(10752, 120)\n",
        "        if input_bins == 128:\n",
        "          self.fc1 = nn.Linear(21808, 120)\n",
        "        self.fc2 = nn.Linear(120, 84)\n",
        "        self.fc3 = nn.Linear(84, 2)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.pool(F.relu(self.conv1(x)))\n",
        "        x = self.pool(F.relu(self.conv2(x)))\n",
        "        x = torch.flatten(x, 1) # flatten all dimensions except batch\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = F.relu(self.fc2(x))\n",
        "        x = self.fc3(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "net = Net()\n",
        "net.to(device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zmglzb_VCkdE"
      },
      "source": [
        "optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2Mvsjrd6CtN_"
      },
      "source": [
        "best_accuracy = 0.0\n",
        "\n",
        "for epoch in range(n_epochs):  # loop over the dataset multiple times\n",
        "\n",
        "    running_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    for i, data in enumerate(trainloader, 0):\n",
        "        # get the inputs; data is a list of [inputs, labels]\n",
        "        inputs, labels = data\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "        # zero the parameter gradients\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # forward + backward + optimize\n",
        "        outputs = net(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # print statistics\n",
        "        running_loss += loss.item()\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "        if i % print_every == 0:    # print every 2000 mini-batches\n",
        "            print('[%d, %5d] loss: %.3f' %\n",
        "                  (epoch + 1, i + 1, running_loss / print_every))\n",
        "            running_loss = 0.0\n",
        "    \n",
        "    print('Accuracy of the network on the train images after epoch {} is {}'.format(epoch, 100 * correct / total) )\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    # since we're not training, we don't need to calculate the gradients for our outputs\n",
        "    with torch.no_grad():\n",
        "        for data in testloader:\n",
        "            inputs, labels = data\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "            # calculate outputs by running images through the network\n",
        "            outputs = net(inputs)\n",
        "            # the class with the highest energy is what we choose as prediction\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "\n",
        "    accuracy_epoch = 100 * correct / total\n",
        "    print('Accuracy of the network on the test images: %d %%' % (\n",
        "        100 * correct / total))\n",
        "    if accuracy_epoch > best_accuracy:\n",
        "      best_accuracy = accuracy_epoch\n",
        "      torch.save(net.state_dict(), os.path.join(BASE_PATH, \"best_model.pt\"))\n",
        "        \n",
        "        \n",
        "    \n",
        "\n",
        "print('Finished Training')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "stk11tO7EzFS"
      },
      "source": [
        "model = Net()\n",
        "model.load_state_dict(torch.load(os.path.join(BASE_PATH, \"best_model.pt\"), map_location=device))\n",
        "model.to(device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NVQKWamEMyOA"
      },
      "source": [
        "device"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bWcjvt23P6Ge"
      },
      "source": [
        "import seaborn as sns\n",
        "# prepare to count predictions for each class\n",
        "correct_pred = {classname: 0 for classname in classes}\n",
        "total_pred = {classname: 0 for classname in classes}\n",
        "num_classes = len(classes)\n",
        "cm = torch.zeros(num_classes, num_classes)\n",
        "\n",
        "# again no gradients needed\n",
        "with torch.no_grad():\n",
        "    for data in testloader:\n",
        "        inputs, labels = data\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "        outputs = model(inputs)\n",
        "        _, predictions = torch.max(outputs, 1)\n",
        "        # collect the correct predictions for each class\n",
        "        for label, prediction in zip(labels, predictions):\n",
        "            if label == prediction:\n",
        "                correct_pred[classes[label]] += 1\n",
        "            total_pred[classes[label]] += 1\n",
        "            cm[label, prediction] = cm[label, prediction] + 1\n",
        "\n",
        "# print accuracy for each class\n",
        "for classname, correct_count in correct_pred.items():\n",
        "    accuracy = 100 * float(correct_count) / total_pred[classname]\n",
        "    print(\"Accuracy for class {:5s} is: {:.1f} %\".format(classname,\n",
        "                                                   accuracy))\n",
        "    \n",
        "fig = plt.figure(figsize=(10, 8))\n",
        "confusionMatrix = cm.numpy()\n",
        "confusionMatrix = confusionMatrix / confusionMatrix.astype(np.float).sum(axis=1, keepdims=True)\n",
        "hmap = sns.heatmap(confusionMatrix , annot=True,\n",
        "          fmt='.2', cmap='Blues', annot_kws={\"size\": 12},xticklabels=[0,1], yticklabels=[0,1])\n",
        "hmap.set_xticklabels(hmap.get_xmajorticklabels(), fontsize=6)\n",
        "hmap.set_yticklabels(hmap.get_ymajorticklabels(), fontsize=6)\n",
        "figure = hmap.get_figure()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DT8k_kMM5N0t"
      },
      "source": [
        "correct_pred = {classname: 0 for classname in classes}\n",
        "total_pred = {classname: 0 for classname in classes}\n",
        "num_classes = len(classes)\n",
        "cm = torch.zeros(num_classes, num_classes)\n",
        "\n",
        "# again no gradients needed\n",
        "with torch.no_grad():\n",
        "    for data in trainloader:\n",
        "        inputs, labels = data\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "        outputs = model(inputs)\n",
        "        _, predictions = torch.max(outputs, 1)\n",
        "        # collect the correct predictions for each class\n",
        "        for label, prediction in zip(labels, predictions):\n",
        "            if label == prediction:\n",
        "                correct_pred[classes[label]] += 1\n",
        "            total_pred[classes[label]] += 1\n",
        "            cm[label, prediction] = cm[label, prediction] + 1\n",
        "\n",
        "# print accuracy for each class\n",
        "for classname, correct_count in correct_pred.items():\n",
        "    accuracy = 100 * float(correct_count) / total_pred[classname]\n",
        "    print(\"Accuracy for class {:5s} is: {:.1f} %\".format(classname,\n",
        "                                                   accuracy))\n",
        "    \n",
        "fig = plt.figure(figsize=(10, 8))\n",
        "confusionMatrix = cm.numpy()\n",
        "confusionMatrix = confusionMatrix / confusionMatrix.astype(np.float).sum(axis=1, keepdims=True)\n",
        "hmap = sns.heatmap(confusionMatrix , annot=True,\n",
        "          fmt='.2', cmap='Blues', annot_kws={\"size\": 12},xticklabels=[0,1], yticklabels=[0,1])\n",
        "hmap.set_xticklabels(hmap.get_xmajorticklabels(), fontsize=6)\n",
        "hmap.set_yticklabels(hmap.get_ymajorticklabels(), fontsize=6)\n",
        "figure = hmap.get_figure()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CttHa5oWuoqO"
      },
      "source": [
        "classes"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xl4jEUu4CI7x"
      },
      "source": [
        "Cross validation code start"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WDunLqhBDmSY"
      },
      "source": [
        "files = []\n",
        "exclusion = [\"098567.npy\",\"098569.npy\", '098565.npy']\n",
        "for r,d, fileList in os.walk(os.path.join(TEST_DATA_BASE_PATH, TEST_DATA_IMG_DIR, \"Western_numpy\")):\n",
        "  for file in fileList:\n",
        "    if(file not in exclusion):\n",
        "      files.append([file,file, \"Western_numpy\"])\n",
        "    else:\n",
        "      print(file)\n",
        "\n",
        "random.seed(1234)\n",
        "files = random.sample(files, 2008)\n",
        "\n",
        "for r,d, fileList in os.walk(os.path.join(TEST_DATA_BASE_PATH, TEST_DATA_IMG_DIR, \"Indian_numpy\")):\n",
        "  for file in fileList:\n",
        "    if(True):\n",
        "      files.append([file,file, \"Indian_numpy\"])\n",
        "    else:\n",
        "      print(file)\n",
        "\n",
        "data = pd.DataFrame(files, columns=['Images','songs','labels'])\n",
        "data.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Oqyjb8tvFZcP"
      },
      "source": [
        "data['encoded_labels'] = lb.transform(data['labels'])\n",
        "data.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tMGfdIofFJOz"
      },
      "source": [
        "cross_dataset = MEL_Dataset(data,os.path.join(TEST_DATA_BASE_PATH, TEST_DATA_IMG_DIR),transform)\n",
        "crossloader = torch.utils.data.DataLoader(cross_dataset, batch_size=batch_size)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "chStH11_Fr05"
      },
      "source": [
        "# get some random training images\n",
        "dataiter = iter(crossloader)\n",
        "images, labels = dataiter.next()\n",
        "arthopod_types = {0: 'Indian_numpy', 1: 'Western_numpy', 2: 'country', 3: 'disco', 4: 'hiphop', 5: 'jazz', 6: 'metal', 7: 'pop', 8: 'reggae', 9: 'rock'}\n",
        "# Viewing data examples used for training\n",
        "fig, axis = plt.subplots(3, 5, figsize=(15, 10))\n",
        "for i, ax in enumerate(axis.flat):\n",
        "    with torch.no_grad():\n",
        "        image, label = images[i], labels[i]\n",
        "        ax.imshow(mel_display(image), interpolation='nearest', aspect='auto') # add image\n",
        "        ax.set(title = f\"{arthopod_types[label.item()]}\") # add label"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YgNDNS8DF2dr"
      },
      "source": [
        "import seaborn as sns\n",
        "correct_pred = {classname: 0 for classname in classes}\n",
        "total_pred = {classname: 0 for classname in classes}\n",
        "num_classes = len(classes)\n",
        "cm = torch.zeros(num_classes, num_classes)\n",
        "\n",
        "# again no gradients needed\n",
        "with torch.no_grad():\n",
        "    for data in crossloader:\n",
        "        inputs, labels = data\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "        outputs = model(inputs)\n",
        "        _, predictions = torch.max(outputs, 1)\n",
        "        # collect the correct predictions for each class\n",
        "        for label, prediction in zip(labels, predictions):\n",
        "            if label == prediction:\n",
        "                correct_pred[classes[label]] += 1\n",
        "            total_pred[classes[label]] += 1\n",
        "            cm[label, prediction] = cm[label, prediction] + 1\n",
        "\n",
        "# print accuracy for each class\n",
        "for classname, correct_count in correct_pred.items():\n",
        "    accuracy = 100 * float(correct_count) / total_pred[classname]\n",
        "    print(\"Accuracy for class {:5s} is: {:.1f} %\".format(classname,\n",
        "                                                   accuracy))\n",
        "    \n",
        "fig = plt.figure(figsize=(10, 8))\n",
        "confusionMatrix = cm.numpy()\n",
        "confusionMatrix = confusionMatrix / confusionMatrix.astype(np.float).sum(axis=1, keepdims=True)\n",
        "hmap = sns.heatmap(confusionMatrix , annot=True,\n",
        "          fmt='.2', cmap='Blues', annot_kws={\"size\": 6},xticklabels=[0,1], yticklabels=[0,1])\n",
        "hmap.set_xticklabels(hmap.get_xmajorticklabels(), fontsize=6)\n",
        "hmap.set_yticklabels(hmap.get_ymajorticklabels(), fontsize=6)\n",
        "figure = hmap.get_figure()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}